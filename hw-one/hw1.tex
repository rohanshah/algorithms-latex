\documentclass[12pt]{article}
\usepackage{fullpage}
\usepackage{titlesec}
\usepackage{tikz}
\usepackage{amsfonts,amssymb}
\usepackage{amsmath}
\usepackage{comment}
\relpenalty=9999
\binoppenalty=9999

\begin{document}
\pagestyle{plain}
\titleformat{\subsection}[runin]
  {\normalfont\large\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}[runin]
  {\normalfont\large\bfseries}{\thesubsubsection}{1em}{}

\title{CIS 677: Homework 1}
\author{Rohan Shah\\Discussed with: Evan Yeatts}
\date{}

\maketitle

\section*{Problem 1.}
For the sets $S_1$ and $S_2$ to correspond to the two sides of the
minimum cut in the graph $G$ the following claims must hold:
\vspace{2mm}
\newline
\textbf{Claim:} At least one edge in $T$ is an edge from the min-cut.
\newline
\textbf{Proof:} If $T$ does not contain an edge from the min-cut then by
definition $T$ is cut into two disjoint sets of nodes which means it is not a
minimum spanning tree, a contradiction.
\vspace{2mm}
\newline
\textbf{Claim:} The heaviest weight edge in the $T$, i.e. the edge that is
removed from $T$ by the algorithm, must be an edge from the min-cut.
\newline
\textbf{Proof:} By the above claim at least on edge in $T$ belongs to the
min-cut. If the edge removed from $T$ is not an edge from the min-cut then
the endpoints ofthe min-cut edge both lie in either $S_1$ or $S_2$ which, by
definition, means $S_1$ and $S_2$ do not correspond to the partitions of the
min-cut.
\vspace{2mm}
\newline
\textbf{Claim:} There must be exactly one edge from the min-cut in $T$.
\newline
\textbf{Proof:} Since only one edge is removed from $T$ at the end of the
algorithm, if more than one edge from the min-cut is in $T$ then by the same
logic as the previous claim $S_1$ and $S_2$ cannot correspond to the partitions
of the min-cut.
\vspace{2mm}
\newline
By the above claims, the probability that $S_1$ and $S_2$ to correspond to the
two sides of the min-cut is equal to the probability that no edge from the
min-cut is chosen to be in $T$ in the first $n-2$ steps of Kruskal's algorithm.
Consider a slight modification to Kruskal's algorithm, where at each step the
minimum weight edge is chosen to be in $T$, all remaining edges who's remaining
endpoints are already connected are removed from the selection pool. This is
safe since those edges could never be chosen to be in $T$ by Kruskal's
algorithm. Further, this algorithm corresponds directly to each step of the
contraction algorith. Since the algorithm assigns weights $w(e)$ to the edges
at random, each step in this modified version of Kruskal's algorithm happens
by choosing an edge from $G$ at random. Thus this algorithm directly corresponds
to the contraction algorith. Since we are looking for the probability that all
edges in the min-cut survive the first $n-2$ edge selections, the analysis is
exactly the same as the one for the contraction algorithm. Thus, the probability
of success is $\Omega(\frac{1}{n^2})$.

\section*{Problem 2.}
If $NP \subseteq BPP$ then there is exists a BPP algorithm for SAT. Without
loss of generality we say this algorithm returns a correct answer with probability
at least $1 - \frac{1}{n^2}$. We can convert this BPP algorithm to an RP
algorithm for SAT in the following way. Choose a variable uniformly at random
and assign its value as true. Then reduce the SAT problem, and run the BPP
algorithm for this smaller SAT problem. If the BPP algorithm returns saying that this smaller
problem is satisfiable then leave the variable assigned as true otherwise assign
it the value false and re-reduce the original SAT problem. Repeat this procedure
until all the variables have been assigned a value. Finally, verify that the
assignment we have obtained satisfies the original SAT problem. If it does then
return true (it is satisfiable) otherwise return false (it is not satisfiable).
In the case where the SAT problem is not satisfiable we always return the correct
answer since we verify our assignment at the end (which in this case where the
problem is not satisifable would always yield false) i.e. the probability we
return that the problem is satisfiable when in fact it is not, is 0, which is what
we need for an RP algorithm. On the other hand, in the case where the problem is
satisfiable, we return the correct answer if and only if we assigned each variable
in a way such that we end up with a satisfying assignment. This is equivalent to
the BPP algorithm returning the correct answer in each iteration. Since the BPP
algorithm returns the correct answer with probability at least
$1 - \frac{1}{n^2}$ and we run it for $n$ variables, the probability that it
returned the correct answer every time is
$(1 - \frac{1}{n^2})^n \ge \frac{1}{2}$. So with probability greater than
$\frac{1}{2}$ our RP algorithm returns the correct answer when the SAT problem
is in fact satisfiable, which is also what is required for an RP algorithm.
Therefore $NP \subseteq RP$.

\section*{Problem 3a.}
\textbf{Claim:} $\textbf{Pr}[Y_S = 0] = \textbf{Pr}[Y_S = 1] = \frac{1}{2}$
i.e. $Y_S$ is uniformly distributed over $\{0,1\}$.
\vspace{2mm}
\newline
\textbf{Proof:} By induction on the cardinality of $S$ where $S \neq \emptyset$.
\newline
\textbf{Base case:} $|S| = 1$. Then $S = \{i\}$ for some $i \in \{0,...,k\}$
so $Y_S = \{X_i\}$.
$$\textbf{Pr}[Y_S = 0] = \textbf{Pr}[X_i = 0] = \frac{1}{2}$$
$$\textbf{Pr}[Y_S = 1] = \textbf{Pr}[X_i = 1] = \frac{1}{2}$$
\newline
\textbf{Inductive case:} $|S| = n$. Then $S = S' \cup \{i\}$ where
$|S'| = n-1$, $S' \neq \emptyset$, and $i \notin S'$. Then
$Y_S = Y_{S'} \oplus X_i$. By the induction hypothesis 
$\textbf{Pr}[Y_{S'} = 0] = \textbf{Pr}[Y_{S'} = 1] = \frac{1}{2}$.
So
$$\textbf{Pr}[Y_S = 0] = \textbf{Pr}[Y_{S'} \oplus X_i = 0] =
\textbf{Pr}[Y_{S'} = 0]\textbf{Pr}[X_i = 0] +
\textbf{Pr}[Y_{S'} = 1]\textbf{Pr}[X_i = 1] = \frac{1}{2}$$
$$\textbf{Pr}[Y_S = 1] = \textbf{Pr}[Y_{S'} \oplus X_i = 1] =
\textbf{Pr}[Y_{S'} = 0]\textbf{Pr}[X_i = 1] +
\textbf{Pr}[Y_{S'} = 1]\textbf{Pr}[X_i = 0] = \frac{1}{2}$$
\newline
We can now prove $\{Y_S\ |\ S\subseteq \{1,...,k\}, S\neq\emptyset \}$ is a
collection of pairwise independent random variables. Since $\oplus$ is
associative and communitive we can write two random variables from the
collection, $Y_S$ and $Y_T$ as
$$Y_S = Y_{S\cap T} \oplus Y_{S/T}$$
$$Y_T = Y_{S\cap T} \oplus Y_{T/S}$$
where $S\cap T$, $S/T$, and $T/S$ are disjoint, which from the above claim means
$Y_{S\cap T}$, $Y_{S/T}$, $Y_{T/S}$ each take a value of $\{0,1\}$ with equal
probability. From the following truth table we get
\begin{center}
\begin{tabular}{| c | c | c || c | c |}
\hline
$Y_{S/T}$ & $Y_{T/S}$ & $Y_{S\cap T}$ & $Y_S$ & $Y_T$ \\ \hline
0 & 0 & 0 & 0 & 0 \\ \hline
0 & 0 & 1 & 1 & 1 \\ \hline
0 & 1 & 0 & 0 & 1 \\ \hline
0 & 1 & 1 & 1 & 0 \\ \hline
1 & 0 & 0 & 1 & 0 \\ \hline
1 & 0 & 1 & 0 & 1 \\ \hline
1 & 1 & 0 & 1 & 1 \\ \hline
1 & 1 & 1 & 0 & 0 \\ \hline
\end{tabular}
\end{center}
$$\textbf{Pr}[Y_S = 0] = \textbf{Pr}[Y_S = 1] = \frac{1}{2}$$
$$\textbf{Pr}[Y_T = 0] = \textbf{Pr}[Y_T = 1] = \frac{1}{2}$$
$$\textbf{Pr}[Y_S = a \wedge Y_T = b] = \textbf{Pr}[Y_S = a]\textbf{Pr}[Y_T = b]
= \frac{1}{4}$$

\section*{Problem 3b.} Let $h \in H$ be a hash function that works in the
following way. Let $X$ be a string of $\log N \log M$ bits chosen uniformly at
random. Given an input $n \in [N]$ take its binary representation, of length
$\log N$, and $\oplus$ it with the first $\log N$ random bits in $X$.
This will generate a new random $\log N$ bit string i.e. where each bit is an
independent binary random variable. Take this bit string and $\oplus$ its bits
together to create a single random bit with uniform distribution over
$\{0,1\}$ from the claim in part (a). Repeat this procedure for each of the
remaining $(\log M - 1)$ $\log N$ bits in $X$ to give $\log M$ bits i.e. an
element in $M$. Now we need to show that such a hash function is pair-wise
independent. But we know that for all $i,j \in [N]$, $h(i)$ and $h(j)$ are
composed of pairwise independent random variables. So if
$h(i) = k_1k_2...k_{\log M}$ and $h(j) = l_1l_2...l_{\log M}$ then $k_i$ and
$l_i$ are pairwise independent random variables for all $1 \le i \le \log M$.
Thus $\textbf{Pr}[h(i) = k \wedge h(j) = l]$ for all $i \neq j \in [N]$ and
$k,l \in [M]$ is equal to $\textbf{Pr}[h(i) = k]\textbf{Pr}[h(j) = l]$ where the
probability of each of those is $\frac{1}{M}$ since both $h(i)$ and $h(j)$
are randomly generated bit strings in $[M]$. So their combined probability is
$\frac{1}{M^2}$ and therefore $H$ is a family of pairwise independent hash
functions.

\section*{Problem 4a.}
Let $t=\epsilon^{-2} \log m$ be the number of elements
sampled from the stream $S$. By definition the median element in $S$ has rank
$\frac{m}{2}$ so let $L$ and $H$ be disjoint subsets of $S$ such that
$$L = \{s_i \mid rank(s_i, S) < (1-\epsilon)\frac{m}{2}\}$$
$$H = \{s_i \mid rank(s_i, S) > (1+\epsilon)\frac{m}{2}\}$$
Let $s_i$ be an element sampled from $S$ uniformly at random 
(with repetition). Then
$$\textbf{Pr}[s_i \in L] = \frac{(1-\epsilon)\frac{m}{2}}{m} =
\frac{(1-\epsilon)}{2}$$
$$\textbf{Pr}[s_i \in H] = \frac{m - (1+\epsilon)\frac{m}{2}}{m} =
\frac{(1-\epsilon)}{2}$$
The probability that the algorithm does not return an $\epsilon$-approximate
median is equal to the probability that greater than or equal to half
elements sampled from $S$ are in $L$, or in $H$. Since $L$ and $H$ are disjoint
and the probability that an element is in $L$ is the same as the probability
that an element is in $H$, the probability that the element returned
is not an $\epsilon$-approximate median is twice the probability that
greater than or equal to half the elements sampled are in $L$.
Let $X = X_1 + ... + X_t$ where $X_i$ is a binary random variable such
that $X_i = 1$ if $s_i \in L$ and $X_i = 0$ otherwise. Since $X_i$ is a binary
random variable, $\textbf{E}[X_i] = \frac{(1-\epsilon)}{2}$ so by linearity of
expectations $\textbf{E}[X] = t\cdot \frac{(1-\epsilon)}{2}$ from which we get
$t = \frac{2\cdot \textbf{E}[X]}{(1-\epsilon)}$. So then
$$\textbf{Pr}[X \ge \frac{t}{2}] =
\textbf{Pr}[X \ge \frac{\frac{2\cdot \textbf{E}[X]}{(1-\epsilon)}}{2}] =
\textbf{Pr}[X \ge \frac{1}{(1-\epsilon)}\cdot \textbf{E}[X]]$$
Since $\epsilon \in (0,\frac{1}{2}]$ we have the inequality that
$\frac{1}{(1-\epsilon)} \ge (1+\epsilon)$ so
$$\textbf{Pr}[X \ge \frac{1}{(1-\epsilon)}\cdot \textbf{E}[X]] \le
\textbf{Pr}[X \ge (1+\epsilon) \cdot \textbf{E}[X]]$$
Since $0 < \epsilon \le 2e - 1$, by Chernoff bounds we have the inequality that
$$\textbf{Pr}[X \ge (1+\epsilon) \cdot \textbf{E}[X]] <
\frac{1}{e^{\frac{\textbf{E}[X]\cdot \epsilon^2}{4}}}$$
$$\frac{\textbf{E}[X]\cdot \epsilon^2}{4} =
\frac{t\cdot \frac{(1-\epsilon)}{2} \cdot \epsilon^2}{4} =
\frac{\epsilon^{-2} \log m \cdot \frac{(1-\epsilon)}{2} \cdot \epsilon^2}{4} =
(\log m) \cdot \frac{(1-\epsilon)}{8}$$
$$\textbf{Pr}[X \ge (1+\epsilon) \cdot \textbf{E}[X]] <
\frac{1}{\text{poly}(m)}$$
So the probability the algorithm returns an $\epsilon$-approximate median is at
least $$1 - \frac{1}{\text{poly}(m)}$$

\section*{Problem 4b.}
If $r \le t$ then the algorithm outputs an $\epsilon$-approximate element of
rank $r$ with probability $1$ since it maintains a list $T$ of the $r$ smallest
elements and outputs the largest element, which is equivalent to the
$r$th smallest element, which by definition has rank $r$. If $r > t$ then the
probability that the element output by the algorithm is not an
$\epsilon$-approximate median is equal to the probability that the rank of
the largest element in $T$ is less than $(1-\epsilon)\cdot r$ or greater than
$(1+\epsilon)\cdot r$. The former only occurs when greater than or equal to
$t$ of the elements sampled have ranks less than $(1-\epsilon)\cdot r$. The
latter only occurs when less than $t$ of the elements sampled have ranks less
than or equal to $(1+\epsilon)\cdot r$. Let $L$ be a subset of $S$ such that
$$L = \{s_i \mid rank(s_i, S) < (1-\epsilon)\cdot r\}$$
$$\textbf{Pr}[s_i \in L] = \frac{(1-\epsilon)\cdot r}{m}$$
Let $X = X_1 + ... + X_m$ where $X_i$ is a binary random variable such
that $X_i = 1$ if $s_i$ was sampled from the $S$ and $s_i \in L$ and
$X_i = 0$ otherwise. Since $X_i$ is a binary random variable
$$\textbf{E}[X_i] = \textbf{Pr}[X_i = 1] = \frac{t}{r}\cdot
\frac{(1-\epsilon)\cdot r}{m} = \frac{t\cdot (1-\epsilon)}{m}$$
so by linearity of expectations $\textbf{E}[X] =
\frac{t\cdot (1-\epsilon)}{m}\cdot m = t\cdot(1-\epsilon)$. From part (a) we
have that
$$\textbf{Pr}[X \ge t] =
\textbf{Pr}[X \ge \frac{1}{(1-\epsilon)}\cdot \textbf{E}[X]] <
\frac{1}{\text{poly}(m)}$$
which is the probability that the former event occurs. For the latter we have
$$L = \{s_i \mid rank(s_i, S) < (1+\epsilon)\cdot r\}$$
$$\textbf{Pr}[s_i \in L] = \frac{(1+\epsilon)\cdot r}{m}$$
$$\textbf{E}[X_i] = \textbf{Pr}[X_i = 1] = \frac{t}{r}\cdot
\frac{(1+\epsilon)\cdot r}{m} = \frac{t\cdot(1+\epsilon)}{m}$$
$$\textbf{E}[X] = m\cdot \frac{t\cdot(1+\epsilon)}{m}= t\cdot (1+\epsilon)$$
$$\textbf{Pr}[X < t] =
\textbf{Pr}[X < \frac{1}{1+\epsilon}\cdot\textbf{E}[X]] \le
\textbf{Pr}[X < (1-\epsilon)\cdot\textbf{E}[X]]$$
$$\textbf{Pr}[X < (1-\epsilon)\cdot\textbf{E}[X]] <
\frac{1}{e^{\frac{\textbf{E}[X]\cdot\epsilon^2}{2}}}
\text{ by Chernoff bounds}$$
$$\textbf{Pr}[X < t] < \frac{1}{m^{12\cdot(1+\epsilon)}} \equiv
\frac{1}{\text{poly}(m)}$$
Therefore the algorithm outputs an $\epsilon$-approximate element of rank $r$
with probability at least
$$1-\frac{1}{\text{poly}(m)}$$

\section*{Problem 5a.}
Given a deterministic algorithm and a tree $T$ the algorithm will always query
the leaves of $T$ in the same order. We can inductively construct an adversary
tree for the algorithm in the following way such that the algorithm is forced to
query all $n = 3^h$ leaves. For the base case where $h = 1$, since the algorithm
is deterministic, we can assign Boolean values to the leaves such that the
first two leaves queried have opposite values, forcing the algorithm to query
all three leaves. Inductively then for a tree of height $h$, by the induction
hypothesis we have that each subtree of height $h-1$ requires querying each of
its $3^(h-1)$ leaves to determine its value. Again we construct our tree such
that the two subtrees who's values are evaluated first have opposite values,
forcing the algorithm to determine the value of the third subtree, meaning the
algorithm needs to compute all $3^h$ leaves.

\section*{Problem 5b.}
Similary we can have an adversary tree for a recursive randomized algorithm that
tries to minimize the chance of finding the majority value of its subtrees
without querying all its leaves. Such a tree would never contain a node whose
three subtrees all had the same value (because that would increase the chance of
discovering that node's value from any two subtrees to 1). So every node in the
adversary tree must have subtrees where exactly two values are equal and one
is the opposite. Since two subtrees are evaluated uniformly at random the
probability that only two subtrees are queried to find the majority is
$\frac{1}{3}$ and the probability that all three subtrees are queried is
$\frac{2}{3}$. So the number of leaves expected to be queried for a tree of
height $h$ is defined by the recurrence relation
$$T(h) = 2\cdot\frac{1}{3}\cdot T(h-1) + 3\cdot\frac{2}{3}\cdot T(h-1)$$
The base case, $T(1)$ has an expected value of querying $\frac{8}{3}$ leaves.
A tree of height $h$ has $\log_3 n$ leaves. So the expected number of leaves
queried by the algorithm on any instance is at most $n^{\log_3 \frac{8}{3}}$.
\end{document}
