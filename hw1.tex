\documentclass[12pt]{article}
\usepackage{fullpage}
\usepackage{titlesec}
\usepackage{tikz}
\usepackage{amsfonts,amssymb}
\usepackage{amsmath}
\usepackage{comment}
\relpenalty=9999
\binoppenalty=9999

\begin{document}
\pagestyle{plain}
\titleformat{\subsection}[runin]
  {\normalfont\large\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}[runin]
  {\normalfont\large\bfseries}{\thesubsubsection}{1em}{}

\title{CIS 677: Homework 1}
\author{Rohan Shah}
\date{}

\maketitle

\section*{Problem 1.}
For the sets $S_1$ and $S_2$ to correspond to the two sides of the
minimum cut in the graph $G$ the following claims must hold:
\vspace{2mm}
\newline
\textbf{Claim:} At least one edge in $T$ is an edge from the min-cut.
\newline
\textbf{Proof:} If $T$ does not contain an edge from the min-cut then by
definition $T$ is cut into two disjoint sets of nodes which means it is not a
minimum spanning tree, a contradiction.
\vspace{2mm}
\newline
\textbf{Claim:} The heaviest weight edge in the $T$, i.e. the edge that is
removed from $T$ by the algorithm, must be an edge from the min-cut.
\newline
\textbf{Proof:} By the above claim at least on edge in $T$ belongs to the
min-cut. If the edge removed from $T$ is not an edge from the min-cut then
the endpoints ofthe min-cut edge both lie in either $S_1$ or $S_2$ which, by
definition, means $S_1$ and $S_2$ do not correspond to the partitions of the
min-cut.
\vspace{2mm}
\newline
\textbf{Claim:} There must be exactly one edge from the min-cut in $T$.
\newline
\textbf{Proof:} Since only one edge is removed from $T$ at the end of the
algorithm, if more than one edge from the min-cut is in $T$ then by the same
logic as the previous claim $S_1$ and $S_2$ cannot correspond to the partitions
of the min-cut.
\vspace{2mm}
\newline
By the above claims, the probability that $S_1$ and $S_2$ to correspond to the
two sides of the min-cut is equal to the probability that no edge from the
min-cut is chosen to be in $T$ in the first $n-2$ steps of Kruskal's algorithm.
Consider a slight modification to Kruskal's algorithm, where at each step the
minimum weight edge is chosen to be in $T$, all remaining edges who's remaining
endpoints are already connected are removed from the selection pool. This is
safe since those edges could never be chosen to be in $T$ by Kruskal's
algorithm. Further, this algorithm corresponds directly to each step of the
contraction algorith. Since the algorithm assigns weights $w(e)$ to the edges
at random, each step in this modified version of Kruskal's algorithm happens
by choosing an edge from $G$ at random. Thus this algorithm directly corresponds
to the contraction algorith. Since we are looking for the probability that all
edges in the min-cut survive the first $n-2$ edge selections, the analysis is
exactly the same as the one for the contraction algorithm. Thus, the probability
of success is $\Omega(\frac{1}{n^2})$.

\newpage
\section*{Problem 2.}
If $NP \subseteq BPP$ then there is exists a BPP algorithm for SAT. Without
loss of generality we say this algorithm returns a correct answer with probability
at least $1 - \frac{1}{n^2}$. We can convert this BPP algorithm to an RP
algorithm for SAT in the following way. Choose a variable uniformly at random
and assign its value as true. Then reduce the SAT problem, and run the BPP
algorithm for this smaller SAT problem. If the BPP algorithm returns saying that this smaller
problem is satisfiable then leave the variable assigned as true otherwise assign
it the value false and re-reduce the original SAT problem. Repeat this procedure
until all the variables have been assigned a value. Finally, verify that the
assignment we have obtained satisfies the original SAT problem. If it does then
return true (it is satisfiable) otherwise return false (it is not satisfiable).
In the case where the SAT problem is not satisfiable we always return the correct
answer since we verify our assignment at the end (which in this case where the
problem is not satisifable would always yield false) i.e. the probability we
return that the problem is satisfiable when in fact it is not, is 0, which is what
we need for an RP algorithm. On the other hand, in the case where the problem is
satisfiable, we return the correct answer if and only if we assigned each variable
in a way such that we end up with a satisfying assignment. This is equivalent to
the BPP algorithm returning the correct answer in each iteration. Since the BPP
algorithm returns the correct answer with probability at least
$1 - \frac{1}{n^2}$ and we ran it for $n$ variables, the probability that it
returned the correct answer every time is
$(1 - \frac{1}{n^2})^n \ge \frac{1}{2}$. So with probability greater than
$\frac{1}{2}$ our RP algorithm returns the correct answer when the SAT problem
is in fact satisfiable, which is what is required for an RP algorithm.
Therefore $NP \subseteq RP$.

\newpage
\section*{Problem 3a.}
\textbf{Claim:} $\textbf{Pr}[Y_S = 0] = \textbf{Pr}[Y_S = 1] = \frac{1}{2}$
i.e. $Y_S$ is uniformly distributed over $\{0,1\}$.
\vspace{2mm}
\newline
\textbf{Proof:} By induction on the cardinality of $S$ where $S \neq \emptyset$.
\newline
\textbf{Base case:} $|S| = 1$. Then $S = \{i\}$ for some $i \in \{0,...,k\}$
so $Y_S = \{X_i\}$.
$$\textbf{Pr}[Y_S = 0] = \textbf{Pr}[X_i = 0] = \frac{1}{2}$$
$$\textbf{Pr}[Y_S = 1] = \textbf{Pr}[X_i = 1] = \frac{1}{2}$$
\newline
\textbf{Inductive case:} $|S| = n$. Then $S = S' \cup \{i\}$ where
$|S'| = n-1$, $S' \neq \emptyset$, and $i \notin S'$. Then
$Y_S = Y_{S'} \oplus X_i$. By the induction hypothesis 
$\textbf{Pr}[Y_{S'} = 0] = \textbf{Pr}[Y_{S'} = 1] = \frac{1}{2}$.
So
$$\textbf{Pr}[Y_S = 0] = \textbf{Pr}[Y_{S'} \oplus X_i = 0] =
\textbf{Pr}[Y_{S'} = 0]\textbf{Pr}[X_i = 0] +
\textbf{Pr}[Y_{S'} = 1]\textbf{Pr}[X_i = 1] = \frac{1}{2}$$
$$\textbf{Pr}[Y_S = 1] = \textbf{Pr}[Y_{S'} \oplus X_i = 1] =
\textbf{Pr}[Y_{S'} = 0]\textbf{Pr}[X_i = 1] +
\textbf{Pr}[Y_{S'} = 1]\textbf{Pr}[X_i = 0] = \frac{1}{2}$$
\newline
We can now prove $\{Y_S\ |\ S\subseteq \{1,...,k\}, S\neq\emptyset \}$ is a
collection of pairwise independent random variables. Since $\oplus$ is
associative and communitive we can write two random variables from the
collection, $Y_S$ and $Y_T$ as
$$Y_S = Y_{S\cap T} \oplus Y_{S/T}$$
$$Y_T = Y_{S\cap T} \oplus Y_{T/S}$$
where $S\cap T$, $S/T$, and $T/S$ are disjoint, which from the above claim means
$Y_{S\cap T}$, $Y_{S/T}$, $Y_{T/S}$ each take a value of $\{0,1\}$ with equal
probability. From the following truth table we get
\begin{center}
\begin{tabular}{| c | c | c || c | c |}
\hline
$Y_{S/T}$ & $Y_{T/S}$ & $Y_{S\cap T}$ & $Y_S$ & $Y_T$ \\ \hline
0 & 0 & 0 & 0 & 0 \\ \hline
0 & 0 & 1 & 1 & 1 \\ \hline
0 & 1 & 0 & 0 & 1 \\ \hline
0 & 1 & 1 & 1 & 0 \\ \hline
1 & 0 & 0 & 1 & 0 \\ \hline
1 & 0 & 1 & 0 & 1 \\ \hline
1 & 1 & 0 & 1 & 1 \\ \hline
1 & 1 & 1 & 0 & 0 \\ \hline
\end{tabular}
\end{center}
$$\textbf{Pr}[Y_S = 0] = \textbf{Pr}[Y_S = 1] = \frac{1}{2}$$
$$\textbf{Pr}[Y_T = 0] = \textbf{Pr}[Y_T = 1] = \frac{1}{2}$$
$$\textbf{Pr}[Y_S = a \wedge Y_T = b] = \textbf{Pr}[Y_S = a]\textbf{Pr}[Y_T = b]
= \frac{1}{4}$$

\newpage
\section*{Problem 3b.} Let $h \in H$ be a has function that works in the following
way. Generate $\log N \log M$ bits uniformly at random. Given an input
$n \in [N]$ take its bitwise representation of length $\log N$ and xor it with
the first $\log N$ random bits previously generated. This will generate a new
random $\log N$ bit string i.e. where each bit is a independent binary random
variable. Take this bit string and xor the bits in this string together to
create a single bit which has probability uniformly distributed over $\{0,1\}$
from part (a). Repeat this procedure for the remaining $(\log M - 1)$ $\log N$
random bits to give a final $\log M$ randomly generated bit string i.e. an
element in $M$. Now we need to show that such a hash function is pair-wise
independent. But we know that for all $i,j \in [N]$, $h(i)$ and $h(j)$ are
composed of pairwise independent random variables. So if
$h(i) = k_1k_2...k_{\log M}$ and $h(j) = l_1l_2...l_{\log M}$ then $k_i$ and
$l_i$ are pairwise independent random variables for all $1 \le i \le \log M$.
Thus $\textbf{Pr}[h(i) = k \wedge h(j) = l]$ for all $i \neq j \in [N]$ and
$k,l \in [M]$ is equal to $\textbf{Pr}[h(i) = k]\textbf{Pr}[h(j) = l]$ where the
probability of each of those is $\frac{1}{M}$ since both $h(i)$ and $h(j)$
are randomly generated bit strings in $[M]$. So their combined probability is
$\frac{1}{M^2}$ and therefore $H$ is a family of pairwise independent hash
functions.

\newpage
\section*{Problem 4a.}
Let $t=\epsilon^{-2} \log m$ be the number of elements
sampled from the stream $S$. By definition the median element in $S$ has rank
$\frac{m}{2}$ so let $L$ and $H$ be disjoint subsets of $S$ such that
$$L = \{s_i \mid rank(s_i, S) < (1-\epsilon)\frac{m}{2}\}$$
$$H = \{s_i \mid rank(s_i, S) > (1+\epsilon)\frac{m}{2}\}$$
Let $s_i$ be an element sampled from $S$ uniformly at random 
(with repetition). Then
$$\textbf{Pr}[s_i \in L] = \frac{(1-\epsilon)\frac{m}{2}}{m} =
\frac{(1-\epsilon)}{2}$$
$$\textbf{Pr}[s_i \in H] = \frac{m - (1+\epsilon)\frac{m}{2}}{m} =
\frac{(1-\epsilon)}{2}$$
The probability that the algorithm does not return an $\epsilon$-approximate
median is equal to the probability that greater than or equal to half
elements sampled from $S$ are in $L$, or in $H$. Since $L$ and $H$ are disjoint
and the probability that an element is in $L$ is the same as the probability
that an element is in $H$, the probability that the element returned
is not an $\epsilon$-approximate median is twice the probability that
greater than or equal to half the elements sampled are in $L$.
Let $X = X_1 + ... + X_t$ where $X_i$ is a binary random variable such
that $X_i = 1$ if $s_i \in L$ and $X_i = 0$ otherwise. Since $X_i$ is a binary
random variable, $\textbf{E}[X_i] = \frac{(1-\epsilon)}{2}$ so by linearity of
expectations $\textbf{E}[X] = t\cdot \frac{(1-\epsilon)}{2}$ from which we get
$t = \frac{2\cdot \textbf{E}[X]}{(1-\epsilon)}$. So then
$$\textbf{Pr}[X \ge \frac{t}{2}] =
\textbf{Pr}[X \ge \frac{\frac{2\cdot \textbf{E}[X]}{(1-\epsilon)}}{2}] =
\textbf{Pr}[X \ge \frac{1}{(1-\epsilon)}\cdot \textbf{E}[X]]$$
Since $\epsilon \in (0,\frac{1}{2}]$ we have the inequality that
$\frac{1}{(1-\epsilon)} \ge (1+\epsilon)$ so
$$\textbf{Pr}[X \ge \frac{1}{(1-\epsilon)}\cdot \textbf{E}[X]] \le
\textbf{Pr}[X \ge (1+\epsilon) \cdot \textbf{E}[X]]$$
Since $0 < \epsilon \le 2e - 1$, by Chernoff bounds we have the inequality that
$$\textbf{Pr}[X \ge (1+\epsilon) \cdot \textbf{E}[X]] <
\frac{1}{e^{\frac{\textbf{E}[X]\cdot \epsilon^2}{4}}}$$
$$\frac{\textbf{E}[X]\cdot \epsilon^2}{4} =
\frac{t\cdot \frac{(1-\epsilon)}{2} \cdot \epsilon^2}{4} =
\frac{\epsilon^{-2} \log m \cdot \frac{(1-\epsilon)}{2} \cdot \epsilon^2}{4} =
(\log m) \cdot \frac{(1-\epsilon)}{8}$$
$$\textbf{Pr}[X \ge (1+\epsilon) \cdot \textbf{E}[X]] <
\frac{1}{\text{poly}(m)}$$
So the probability the algorithm returns an $\epsilon$-approximate median is at
least $$1 - \frac{1}{\text{poly}(m)}$$

\newpage
\section*{Problem 4b.}
If $r \le t$ then the algorithm outputs an $\epsilon$-approximate element of
rank $r$ with probability $1$ since it maintains a list $T$ of the $r$ smallest
elements and outputs the largest element, which is equivalent to the
$r$th smallest element, which by definition has rank $r$. If $r > t$ then the
probability that the element output by the algorithm is not an
$\epsilon$-approximate median is equal to the probability that the rank of
the largest element in $T$ is less than $(1-\epsilon)\cdot r$ or greater than
$(1+\epsilon)\cdot r$. The former only occurs when greater than or equal to
$t$ of the elements sampled have ranks less than $(1-\epsilon)\cdot r$. The
latter only occurs when less than $t$ of the elements sampled have ranks less
than or equal to $(1+\epsilon)\cdot r$. Let $L$ be a subset of $S$ such that
$$L = \{s_i \mid rank(s_i, S) < (1-\epsilon)\cdot r\}$$
$$\textbf{Pr}[s_i \in L] = \frac{(1-\epsilon)\cdot r}{m}$$
Let $X = X_1 + ... + X_m$ where $X_i$ is a binary random variable such
that $X_i = 1$ if $s_i$ was sampled from the $S$ and $s_i \in L$ and
$X_i = 0$ otherwise. Since $X_i$ is a binary random variable
$$\textbf{E}[X_i] = \textbf{Pr}[X_i = 1] = \frac{t}{r}\cdot
\frac{(1-\epsilon)\cdot r}{m} = \frac{t\cdot (1-\epsilon)}{m}$$
so by linearity of expectations $\textbf{E}[X] =
\frac{t\cdot (1-\epsilon)}{m}\cdot m = t\cdot(1-\epsilon)$. From part (a) we
have that
$$\textbf{Pr}[X \ge t] =
\textbf{Pr}[X \ge \frac{1}{(1-\epsilon)}\cdot \textbf{E}[X]] <
\frac{1}{\text{poly}(m)}$$
which is the probability that the former event occurs. For the latter we have
$$L = \{s_i \mid rank(s_i, S) < (1+\epsilon)\cdot r\}$$
$$\textbf{Pr}[s_i \in L] = \frac{(1+\epsilon)\cdot r}{m}$$
$$\textbf{E}[X_i] = \textbf{Pr}[X_i = 1] = \frac{t}{r}\cdot
\frac{(1+\epsilon)\cdot r}{m} = \frac{t\cdot(1+\epsilon)}{m}$$
$$\textbf{E}[X] = m\cdot \frac{t\cdot(1+\epsilon)}{m}= t\cdot (1+\epsilon)$$
$$\textbf{Pr}[X < t] =
\textbf{Pr}[X < \frac{1}{1+\epsilon}\cdot\textbf{E}[X]] \le
\textbf{Pr}[X < (1-\epsilon)\cdot\textbf{E}[X]]$$
$$\textbf{Pr}[X < (1-\epsilon)\cdot\textbf{E}[X]] <
\frac{1}{e^{\frac{\textbf{E}[X]\cdot\epsilon^2}{2}}}
\text{ by Chernoff bounds}$$
$$\textbf{Pr}[X < t] < \frac{1}{m^{12\cdot(1+\epsilon)}} \equiv
\frac{1}{\text{poly}(m)}$$
Therefore the algorithm outputs an $\epsilon$-approximate element of rank $r$
with probability at least
$$1-\frac{1}{\text{poly}(m)}$$

\newpage
\section*{Problem 5a.}

\newpage
\section*{Problem 5b.}

\end{document}
